{"paragraphs":[{"text":"%pyspark\nfrom pyspark.mllib.feature import StandardScaler\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark import SparkContext\nimport subprocess\n\n#remove existing copies of dataset from HDFS\n#subprocess.call([\"hadoop\",\"fs\",\"-rm\",\"-r\",\"-f\",\"/tmp/boston-data/\"])#delete directory\n#subprocess.call([\"hadoop\",\"fs\",\"-mkdir\",\"/tmp/boston-data/\"])#create directory\n\n#subprocess.call([\"hadoop\",\"fs\",\"-put\",\"/tmp/boston/boston_house.csv\",\"/tmp/boston-data/\"])#transfer files\n#subprocess.call([\"hadoop\",\"fs\",\"-put\",\"/tmp/boston/verification.csv\",\"/tmp/boston-data/\"])#transfer files\n#subprocess.call([\"hadoop\",\"fs\",\"-ls\",\"-h\",\"/tmp/boston/\"])#list the files\n#subprocess.call([\"hadoop\",\"fs\",\"-ls\",\"-h\",\"/tmp/boston-data/\"])#list the files\n#hadoop fs -rm -r -f /tmp/boston-data/predicted1_results\n\n\nTrainFile = sc.textFile(\"hdfs://sandbox.hortonworks.com/tmp/boston-data/boston_house.csv\")\n#remove the header\nfileHeader = TrainFile.first()#fileHeader contains only the headers\nTrainFileNoHeader = TrainFile.filter(lambda line:line != fileHeader)#takes all but the header and puts in the variable TrainFileNoHeader\nTrainFileSplit = TrainFileNoHeader.map(lambda line: line.split(\",\")) #separate each feature. For each line/record/row, the cells are separated by a comma #now we work with the features i.e. the input \nTrainFeatures = TrainFileSplit.map(lambda row: row[:-1])#take all the features, from the beginning to the penultimate feature i.e. excluding the output label. Still in RDD format\nlabel = TrainFileSplit.map(lambda row: row[-1]) #take the last feature..the label. Map each row in the features RDD to the corresponding row int he output label RDD\nstandardizer = StandardScaler(withMean=True, withStd=True) #create a standard scaler that will scale with the mean and standard deviation\nscaler = standardizer.fit(TrainFeatures) #fit the scaler with the data we are trying to scale\nscaledFeatures = scaler.transform(TrainFeatures)# then transfom the data to the scaled equivalent\ntransTrain = label.zip(scaledFeatures)#this binds the label and the features together so it can be mapped accordingly \ntransTrain = transTrain.map(lambda row: LabeledPoint(row[0],[row[1]]))\n#map the fatures-label to a labledpoint format. Since we had zipped te features to the labels in the previous line, labels are in index 0 and features in index 1\n\n#with our training data done, \nlinearReg = LinearRegressionWithSGD.train(transTrain, intercept=True) #regress the data\n\n#now its time to test using the verification data\ntestData = sc.textFile(\"hdfs://sandbox.hortonworks.com/tmp/boston-data/verification.csv\")\n#remove the header\ntestDataHead = testData.first()\ntestDataNoHeader = testData.filter(lambda line: line != testDataHead)\ntestDataSplit = testDataNoHeader.map(lambda line: line.split(\",\")) #split the data\n#standardize the test data\ntransTest = scaler.transform(testDataSplit)# \n#using the same scaler used for the training datam scale this dta\nrslt = linearReg.predict(transTest)\n\nsubprocess.call([\"hadoop\",\"fs\",\"-rm\",\"-r\",\"-f\",\"/tmp/boston-data/predicted_results\"])#delete directory\nrslt.saveAsTextFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/predicted_results/')\n\n#rslt.saveAsTextFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/predicted_results');","dateUpdated":"2016-10-06T18:43:44+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475723723375_-772727473","id":"20161006-031523_2030801359","result":{"code":"SUCCESS","type":"TEXT","msg":"16/10/06 18:44:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/boston-data/predicted_results' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/boston-data/predicted_results1475779442915\n"},"dateCreated":"2016-10-06T03:15:23+0000","dateStarted":"2016-10-06T18:43:44+0000","dateFinished":"2016-10-06T18:44:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:280","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475724931501_702615102","id":"20161006-033531_106597192","dateCreated":"2016-10-06T03:35:31+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:281"}],"name":"Assignment4","id":"2BXVB6TYX","angularObjects":{"2BW4S5NJ4:shared_process":[],"2BY1Y2HJG:shared_process":[],"2BVYH21ZJ:shared_process":[],"2BV6637D6:shared_process":[],"2BX5CPDSX:shared_process":[],"2BWHGGFZJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}